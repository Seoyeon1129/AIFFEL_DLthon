{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4312fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, MultiHeadAttention, Dropout, LayerNormalization, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bab9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('젊은이', 0.6494427919387817),\n",
       " ('여인', 0.6287257671356201),\n",
       " ('백성', 0.6063710451126099),\n",
       " ('포졸', 0.6043275594711304),\n",
       " ('죄인', 0.5960500836372375),\n",
       " ('선비', 0.5868039131164551),\n",
       " ('부녀자', 0.5654411315917969),\n",
       " ('죄수', 0.5639811754226685),\n",
       " ('구경꾼', 0.5620019435882568),\n",
       " ('손님', 0.5589558482170105)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gensim으로 Word2Vec 모델 로드\n",
    "word2vec_model = gensim.models.Word2Vec.load('ko.bin')\n",
    "\n",
    "word2vec_model.wv.most_similar(\"사람\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57e85c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2533d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터의 크기와 어휘 크기를 가져옵니다.\n",
    "embedding_matrix = word2vec_model.vectors\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "vocab_size = len(word2vec_model.key_to_index) + 1  # 0 인덱스를 위해 1을 더합니다.\n",
    "\n",
    "print('embedding dimension: ', embedding_dim)\n",
    "print('vocab_size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03a6473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 1,486,148\n",
      "Trainable params: 1,486,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 양방향 LSTM 모델\n",
    "\n",
    "# 임베딩 차원과 어휘 사전의 크기는 임의로 설정\n",
    "# 실제 적용 시 데이터에 맞게 조정 필요\n",
    "embedding_dim = 128\n",
    "vocab_size = 10000  # 어휘 사전의 크기 (임의로 설정한 값)\n",
    "\n",
    "# 모델의 입력 정의: 이 예제에서는 임의의 길이의 시퀀스를 처리할 수 있도록 None을 사용\n",
    "sequence_input = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# 임베딩 레이어\n",
    "embedded_sequences = Embedding(vocab_size, embedding_dim)(sequence_input)\n",
    "\n",
    "# 양방향 LSTM 레이어 2개를 쌓음\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)  # 첫 번째 LSTM\n",
    "x = Bidirectional(LSTM(64))(x)  # 두 번째 LSTM\n",
    "\n",
    "# Dense 레이어\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# 출력 레이어: 4개의 클래스에 대한 예측을 출력\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# 최종 모델 생성\n",
    "model_bilstm = Model(sequence_input, predictions)\n",
    "\n",
    "# 모델 컴파일\n",
    "model_bilstm.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 구조 요약 출력\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59db9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 512, 128)     1280000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 512, 128)     263808      embedding_1[0][0]                \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 128)     0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 512, 128)     0           embedding_1[0][0]                \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 512, 128)     256         tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512, 128)     16512       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 128)     0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 512, 128)     0           layer_normalization[0][0]        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512, 128)     256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,577,860\n",
      "Trainable params: 1,577,860\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dropout, LayerNormalization, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 임베딩 차원과 어휘 사전의 크기, 시퀀스의 최대 길이는 임의로 설정\n",
    "# 실제 적용 시 데이터에 맞게 조정 필요\n",
    "embedding_dim = 128\n",
    "vocab_size = 10000  # 어휘 사전의 크기 (임의로 설정한 값)\n",
    "max_seq_length = 512  # 시퀀스의 최대 길이 (임의로 설정한 값)\n",
    "num_heads = 4  # 멀티 헤드 어텐션에서의 헤드 수 (임의로 설정한 값)\n",
    "ff_dim = 128  # 피드포워드 네트워크의 차원 수 (임의로 설정한 값)\n",
    "\n",
    "# 모델의 입력 정의\n",
    "sequence_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# 임베딩 레이어\n",
    "embedded_sequences = Embedding(vocab_size, embedding_dim)(sequence_input)\n",
    "\n",
    "# 트랜스포머 블록\n",
    "# 멀티 헤드 어텐션\n",
    "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(embedded_sequences, embedded_sequences)\n",
    "attention_output = Dropout(0.1)(attention_output)\n",
    "attention_output = LayerNormalization(epsilon=1e-6)(embedded_sequences + attention_output)\n",
    "\n",
    "# 피드포워드 네트워크\n",
    "ffn_output = Dense(ff_dim, activation='relu')(attention_output)\n",
    "ffn_output = Dropout(0.1)(ffn_output)\n",
    "sequence_output = LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "# 글로벌 평균 풀링을 사용하여 시퀀스 차원을 축소\n",
    "pooled_output = GlobalAveragePooling1D()(sequence_output)\n",
    "\n",
    "# Dense 레이어\n",
    "x = Dense(128, activation='relu')(pooled_output)\n",
    "\n",
    "# 출력 레이어: 4개의 클래스에 대한 예측을 출력\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# 최종 모델 생성\n",
    "model_tr = Model(sequence_input, predictions)\n",
    "\n",
    "# 모델 컴파일\n",
    "model_tr.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 구조 요약 출력\n",
    "model_tr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ad3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
